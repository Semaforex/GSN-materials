{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_PFYFWYr4up"
      },
      "source": [
        "#Linear Probe\n",
        "\n",
        "#YOU DO NOT NEED GPU FOR THIS TASK, dont worry.\n",
        "\n",
        "In this task, you will implement a training of linear probe.\n",
        "Probing is a technique of checking if model has a specific information encoded in its hidden state. The simplest way to do it is to train a `probe`, different model that gets hidden state of the original model as input and tries to predict a specific information about this input (e.g. is the word currently processed a noun?). The simplest probe is a linear model.\n",
        "In this notebook, we provide implementation of a simple model consisting of\n",
        "\n",
        "1.   Embedding\n",
        "2.   Two attention layers\n",
        "3.   Classification head\n",
        "\n",
        "This model is trained on a specific task:\n",
        "\n",
        "\n",
        "1.   The input to the model is a sequence with exactly one 0 inside (e.g. 1,2,0,4,5,6)\n",
        "2.   The model should predict the first number after 0 (you can assume that 0 is not on the last position) (in the example, it should be 4)\n",
        "3. Output of the model is the output of the classification head on the last number in the sequence (in this case, 6)\n",
        "\n",
        "Your task is to:\n",
        "\n",
        "\n",
        "\n",
        "1.  (2p) Complete the `generate_batch` for the training specified above and run the training (should reach >90 % accuracy with the provided config, you don't need GPU)\n",
        "2.  (2p) Complete the `get_model_embedding_at_positions` and `get_first_attention_output_at_positions` functions for the probe training.\n",
        "3.  (6p) Write the training of the probe (complete `train_probe`). Probe should be trained to answer a task: \"is the token immediately before the current token 0?\" (e.g. in sequence 1, 2, 0, 4, 5, the probe should output number < 0 for \"2\",\"5\" and > 0 for \"4\"). The probe should be a trained using BCEWithLogitsLoss.\n",
        "It is up to you how you will sample negative examples for probe training, but you should sample them in the way that balances the number of positive and negative examples.\n",
        "You should at least log the accuracy during training. Probe training should reach >90% accuracy when training on the output of the first attention, but should stay at ~50-60% when trained on the output of the embedding (as output of embedding has no information of the previous tokens)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l1BxK1Uq-U2c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzEXXu5C_S4V",
        "outputId": "3aaf211c-67f3-427b-8450-a783ff71047c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 100/2000, Loss: 4.999610424041748\n",
            "Epoch 100/2000, Accuracy: 0.0\n",
            "Epoch 200/2000, Loss: 4.955053806304932\n",
            "Epoch 200/2000, Accuracy: 0.0\n",
            "Epoch 300/2000, Loss: 4.854390621185303\n",
            "Epoch 300/2000, Accuracy: 0.0\n",
            "Epoch 400/2000, Loss: 4.742520809173584\n",
            "Epoch 400/2000, Accuracy: 0.015625\n",
            "Epoch 500/2000, Loss: 4.804018020629883\n",
            "Epoch 500/2000, Accuracy: 0.015625\n",
            "Epoch 600/2000, Loss: 4.85786247253418\n",
            "Epoch 600/2000, Accuracy: 0.015625\n",
            "Epoch 700/2000, Loss: 4.441585063934326\n",
            "Epoch 700/2000, Accuracy: 0.15625\n",
            "Epoch 800/2000, Loss: 2.950584650039673\n",
            "Epoch 800/2000, Accuracy: 0.25\n",
            "Epoch 900/2000, Loss: 0.8388757109642029\n",
            "Epoch 900/2000, Accuracy: 0.859375\n",
            "Epoch 1000/2000, Loss: 0.17074620723724365\n",
            "Epoch 1000/2000, Accuracy: 1.0\n",
            "Epoch 1100/2000, Loss: 0.055065739899873734\n",
            "Epoch 1100/2000, Accuracy: 1.0\n",
            "Epoch 1200/2000, Loss: 0.025743212550878525\n",
            "Epoch 1200/2000, Accuracy: 1.0\n",
            "Epoch 1300/2000, Loss: 0.015844233334064484\n",
            "Epoch 1300/2000, Accuracy: 1.0\n",
            "Epoch 1400/2000, Loss: 0.012686382047832012\n",
            "Epoch 1400/2000, Accuracy: 1.0\n",
            "Epoch 1500/2000, Loss: 0.05425348877906799\n",
            "Epoch 1500/2000, Accuracy: 0.984375\n",
            "Epoch 1600/2000, Loss: 0.00924199353903532\n",
            "Epoch 1600/2000, Accuracy: 1.0\n",
            "Epoch 1700/2000, Loss: 0.00987866148352623\n",
            "Epoch 1700/2000, Accuracy: 1.0\n",
            "Epoch 1800/2000, Loss: 0.004634323995560408\n",
            "Epoch 1800/2000, Accuracy: 1.0\n",
            "Epoch 1900/2000, Loss: 0.24058540165424347\n",
            "Epoch 1900/2000, Accuracy: 0.984375\n",
            "Epoch 2000/2000, Loss: 0.005136243533343077\n",
            "Epoch 2000/2000, Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super(Attention, self).__init__()\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.n_heads = n_heads\n",
        "        self.d_model = d_model\n",
        "        self.head_dim = d_model\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "        self.attention_mechanism = nn.MultiheadAttention(d_model, n_heads, bias=False, batch_first=True)\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = self.norm(x)\n",
        "        q = self.w_q(x)\n",
        "        k = self.w_k(x)\n",
        "        v = self.w_v(x)\n",
        "        mask = torch.triu(torch.ones(q.size(1), q.size(1)), diagonal=1).bool().to(x.device)\n",
        "        attention_output, _ = self.attention_mechanism(q, k, v, attn_mask = mask, is_causal=True)\n",
        "        return attention_output + residual\n",
        "\n",
        "class FullEncoding(nn.Module):\n",
        "    def __init__(self, d_model, vocab_size, seq_length):\n",
        "        super(FullEncoding, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.seq_length = seq_length\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.position_embedding = nn.Embedding(seq_length, d_model)\n",
        "    def forward(self, x):\n",
        "        token_embedding = self.token_embedding(x)\n",
        "        position_embedding = self.position_embedding(torch.arange(x.size(1)).unsqueeze(0).to(x.device))\n",
        "        return token_embedding + position_embedding\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MultiAttentionModel(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, num_layers, seq_length):\n",
        "        super(MultiAttentionModel, self).__init__()\n",
        "        self.full_embedding = FullEncoding(d_model, vocab_size, seq_length)\n",
        "        self.layers = nn.ModuleList([\n",
        "            Attention(d_model, num_heads) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.head = nn.Linear(d_model, vocab_size)\n",
        "    def forward(self, x):\n",
        "        x = self.full_embedding(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return self.head(x)\n",
        "\n",
        "def generate_sequences(vocab_size, batch_size, sequence_length):\n",
        "    return torch.randint(1, vocab_size, (batch_size, sequence_length))\n",
        "\n",
        "def generate_0_placements(batch_size, sequence_length):\n",
        "    return torch.randint(0, sequence_length-1, (batch_size,))\n",
        "\n",
        "def generate_batch(sequences, placements):\n",
        "    \"\"\"\n",
        "    This function, given sequences and placements of 0 tokens, should\n",
        "    generate a batch for MultiAttentionModel\n",
        "    sequences: torch tensor of shape (batch_size, sequence_length)\n",
        "    placements: torch tensor of shape (batch_size,)\n",
        "    return:\n",
        "    sequences_with_0: torch tensor of shape (batch_size, sequence_length), which\n",
        "    is the same as sequences but with 0 at the given placements\n",
        "    targets: torch tensor of shape (batch_size,), which is the number after\n",
        "    0 in the sequences\n",
        "    \"\"\"\n",
        "    batch_size = sequences.size(0)\n",
        "    sequences_with_0 = sequences.clone()\n",
        "    # Task 1\n",
        "\n",
        "    indices = torch.arange(batch_size)\n",
        "\n",
        "    sequences_with_0[indices, placements] = 0\n",
        "    targets = sequences[indices, placements+1]\n",
        "    # End Task 1\n",
        "\n",
        "    return sequences_with_0, targets\n",
        "\n",
        "example_sequences = torch.tensor(\n",
        "    [\n",
        "        [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "        [11, 12, 13, 14, 15, 16, 17, 18, 19, 20],\n",
        "    ]\n",
        ")\n",
        "example_placements = torch.tensor([2, 7])\n",
        "example_targets = torch.tensor([4, 19])\n",
        "example_sequences_with_0 = torch.tensor(\n",
        "    [\n",
        "        [1, 2, 0, 4, 5, 6, 7, 8, 9, 10],\n",
        "        [11, 12, 13, 14, 15, 16, 17, 0, 19, 20],\n",
        "    ]\n",
        ")\n",
        "assert torch.all(generate_batch(example_sequences, example_placements)[0] == example_sequences_with_0)\n",
        "assert torch.all(generate_batch(example_sequences, example_placements)[1] == example_targets)\n",
        "\n",
        "\n",
        "\n",
        "def train(model, optimizer, batch_size, sequence_length, num_epochs, device):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        sequences = generate_sequences(vocab_size, batch_size, sequence_length)\n",
        "        placements = generate_0_placements(batch_size, sequence_length)\n",
        "        batch, target = generate_batch(sequences, placements)\n",
        "        batch = batch.to(device)\n",
        "        target = target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch)\n",
        "        last_token_logits = output[:, -1, :]\n",
        "        loss = criterion(last_token_logits, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if (epoch+1) % 100 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
        "            predicted_token = torch.argmax(last_token_logits, dim=1)\n",
        "            accuracy = (predicted_token == target).float().mean().item()\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}, Accuracy: {accuracy}\")\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 64\n",
        "sequence_length = 10\n",
        "vocab_size = 128\n",
        "d_model = 16\n",
        "num_heads = 1\n",
        "num_layers = 2\n",
        "num_epochs = 2000\n",
        "learning_rate = 0.001\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(0)\n",
        "model = MultiAttentionModel(vocab_size, d_model, num_heads, num_layers, sequence_length)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "train(model, optimizer, batch_size, sequence_length, num_epochs, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "g-lfmQeYGeGD"
      },
      "outputs": [],
      "source": [
        "def get_model_embedding_at_positions(model: MultiAttentionModel, x, positions):\n",
        "    \"\"\"\n",
        "    model: MultiAttentionModel\n",
        "    x: torch tensor of shape (batch_size, sequence_length)\n",
        "    positions: torch tensor of shape (batch_size,)\n",
        "\n",
        "    Example: if model on x has embedding '[[[0, 1], [2, 3], [4, 5]], [[6, 7], [8, 9], [10, 11]]]'\n",
        "    and positions is [0, 2] then the output should be '[[0, 1], [10, 11]]'\n",
        "    return: torch tensor of shape (batch_size, d_model)\n",
        "\n",
        "    \"\"\"\n",
        "    # Task 2.1\n",
        "    batch_size = x.size(0)\n",
        "    all_embeddings = model.full_embedding(x)\n",
        "    indices = torch.arange(batch_size, device=x.device)\n",
        "    chosen_embeddings = all_embeddings[indices, positions]\n",
        "\n",
        "    return chosen_embeddings\n",
        "    # End Task 2.1\n",
        "\n",
        "# batch, target = generate_batch(example_sequences, example_placements)\n",
        "# batch_size = batch.size(0)\n",
        "# print(get_model_embedding_at_positions(model, batch, torch.arange(batch_size)))\n",
        "\n",
        "def get_first_attention_output_at_positions(model, x, positions):\n",
        "    \"\"\"\n",
        "    model: MultiAttentionModel\n",
        "    x: torch tensor of shape (batch_size, sequence_length)\n",
        "    positions: torch tensor of shape (batch_size,)\n",
        "\n",
        "    Example: if first attention of model on x outputs '[[[0, 1], [2, 3], [4, 5]], [[6, 7], [8, 9], [10, 11]]]'\n",
        "    and positions is [0, 2] then the output should be '[[0, 1], [10, 11]]'\n",
        "    return: torch tensor of shape (batch_size, d_model)\n",
        "    \"\"\"\n",
        "    # Task 2.2\n",
        "    batch_size = x.size(0)\n",
        "    x = model.full_embedding(x)\n",
        "    all_attentions = model.layers[0](x)\n",
        "    indices = torch.arange(batch_size, device=x.device)\n",
        "    chosen_attentions = all_attentions[indices, positions]\n",
        "\n",
        "    return chosen_attentions\n",
        "    # End Task 2.2\n",
        "\n",
        "# batch, target = generate_batch(example_sequences, example_placements)\n",
        "# batch_size = batch.size(0)\n",
        "# print(get_first_attention_output_at_positions(model, batch, torch.arange(batch_size)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EDE1X93L56G",
        "outputId": "81a3aadc-a2c6-431d-edf0-d7c692f3f98e"
      },
      "outputs": [],
      "source": [
        "def train_probe(model, probe, optimizer, batch_size, sequence_length, num_epochs, device, model_latent_function):\n",
        "    \"\"\"\n",
        "    Implement the training of the probe.\n",
        "    model_latent_function: either get_model_embedding_at_positions or get_first_attention_output_at_positions\n",
        "    \"\"\"\n",
        "    # Task 3\n",
        "    model = model.to(device)\n",
        "    probe = probe.to(device)\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    for epoch in range(num_epochs):\n",
        "        probe.train()\n",
        "        sequences = generate_sequences(vocab_size, batch_size, sequence_length).to(device)\n",
        "        placements = generate_0_placements(batch_size, sequence_length).to(device)\n",
        "\n",
        "\n",
        "\n",
        "        batch, _ = generate_batch(sequences, placements)\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        positive_instances = model_latent_function(model, batch, placements+1).to(device)\n",
        "        positive_targets = torch.ones(batch_size, device=device)\n",
        "\n",
        "        offsets = torch.randint(low=1, high=sequence_length-1, size=(batch_size, ), device=device)\n",
        "        incorrect_placements = (placements+1+offsets) % sequence_length\n",
        "\n",
        "        negative_instances = model_latent_function(model, batch, incorrect_placements).to(device)\n",
        "        negative_targets = torch.zeros(batch_size, device=device)\n",
        "\n",
        "        double_batch = torch.concatenate((positive_instances, negative_instances))\n",
        "        double_target = torch.concatenate((positive_targets, negative_targets))\n",
        "\n",
        "        batch_indices = torch.randperm(2*batch_size, device=device)[:batch_size]\n",
        "\n",
        "        batch = double_batch[batch_indices]\n",
        "        target = double_target[batch_indices]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = probe(batch)\n",
        "        loss = criterion(output.squeeze(), target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if (epoch+1) % 100 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
        "            predictions = (output.squeeze() > 0).float()\n",
        "            accuracy = (predictions == target).float().mean().item()\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}, Accuracy: {accuracy}\")\n",
        "\n",
        "    # End Task 3\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVxz5RKmRlTP",
        "outputId": "9e1a383a-64d3-4235-de67-ef17e3fb598e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 100/2000, Loss: 0.7376176118850708\n",
            "Epoch 100/2000, Accuracy: 0.484375\n",
            "Epoch 200/2000, Loss: 0.6592475771903992\n",
            "Epoch 200/2000, Accuracy: 0.578125\n",
            "Epoch 300/2000, Loss: 0.6951236724853516\n",
            "Epoch 300/2000, Accuracy: 0.453125\n",
            "Epoch 400/2000, Loss: 0.7018994092941284\n",
            "Epoch 400/2000, Accuracy: 0.4375\n",
            "Epoch 500/2000, Loss: 0.7027159929275513\n",
            "Epoch 500/2000, Accuracy: 0.421875\n",
            "Epoch 600/2000, Loss: 0.6955924034118652\n",
            "Epoch 600/2000, Accuracy: 0.46875\n",
            "Epoch 700/2000, Loss: 0.6729242205619812\n",
            "Epoch 700/2000, Accuracy: 0.46875\n",
            "Epoch 800/2000, Loss: 0.6636433601379395\n",
            "Epoch 800/2000, Accuracy: 0.5625\n",
            "Epoch 900/2000, Loss: 0.6671609878540039\n",
            "Epoch 900/2000, Accuracy: 0.5625\n",
            "Epoch 1000/2000, Loss: 0.6507031321525574\n",
            "Epoch 1000/2000, Accuracy: 0.546875\n",
            "Epoch 1100/2000, Loss: 0.686193585395813\n",
            "Epoch 1100/2000, Accuracy: 0.484375\n",
            "Epoch 1200/2000, Loss: 0.6740529537200928\n",
            "Epoch 1200/2000, Accuracy: 0.5\n",
            "Epoch 1300/2000, Loss: 0.7059669494628906\n",
            "Epoch 1300/2000, Accuracy: 0.46875\n",
            "Epoch 1400/2000, Loss: 0.6604623794555664\n",
            "Epoch 1400/2000, Accuracy: 0.59375\n",
            "Epoch 1500/2000, Loss: 0.706922173500061\n",
            "Epoch 1500/2000, Accuracy: 0.4375\n",
            "Epoch 1600/2000, Loss: 0.6691039800643921\n",
            "Epoch 1600/2000, Accuracy: 0.546875\n",
            "Epoch 1700/2000, Loss: 0.683849573135376\n",
            "Epoch 1700/2000, Accuracy: 0.484375\n",
            "Epoch 1800/2000, Loss: 0.6953441500663757\n",
            "Epoch 1800/2000, Accuracy: 0.453125\n",
            "Epoch 1900/2000, Loss: 0.6969621777534485\n",
            "Epoch 1900/2000, Accuracy: 0.5\n",
            "Epoch 2000/2000, Loss: 0.7209979891777039\n",
            "Epoch 2000/2000, Accuracy: 0.46875\n"
          ]
        }
      ],
      "source": [
        "probe = nn.Linear(d_model, 1)\n",
        "optimizer = torch.optim.Adam(probe.parameters(), lr=learning_rate)\n",
        "\n",
        "train_probe(model, probe, optimizer, batch_size, sequence_length, num_epochs, device, get_model_embedding_at_positions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iwZRHe9RkIi",
        "outputId": "49d135e4-4827-4038-e10d-c4a905a47d18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 100/2000, Loss: 0.3288019895553589\n",
            "Epoch 100/2000, Accuracy: 0.890625\n",
            "Epoch 200/2000, Loss: 0.20135006308555603\n",
            "Epoch 200/2000, Accuracy: 1.0\n",
            "Epoch 300/2000, Loss: 0.1499752402305603\n",
            "Epoch 300/2000, Accuracy: 0.984375\n",
            "Epoch 400/2000, Loss: 0.12741243839263916\n",
            "Epoch 400/2000, Accuracy: 0.96875\n",
            "Epoch 500/2000, Loss: 0.12439523637294769\n",
            "Epoch 500/2000, Accuracy: 0.96875\n",
            "Epoch 600/2000, Loss: 0.12360329926013947\n",
            "Epoch 600/2000, Accuracy: 0.96875\n",
            "Epoch 700/2000, Loss: 0.06919854879379272\n",
            "Epoch 700/2000, Accuracy: 1.0\n",
            "Epoch 800/2000, Loss: 0.09740914404392242\n",
            "Epoch 800/2000, Accuracy: 0.96875\n",
            "Epoch 900/2000, Loss: 0.057605065405368805\n",
            "Epoch 900/2000, Accuracy: 0.984375\n",
            "Epoch 1000/2000, Loss: 0.12252090126276016\n",
            "Epoch 1000/2000, Accuracy: 0.96875\n",
            "Epoch 1100/2000, Loss: 0.09889624267816544\n",
            "Epoch 1100/2000, Accuracy: 0.953125\n",
            "Epoch 1200/2000, Loss: 0.04261656105518341\n",
            "Epoch 1200/2000, Accuracy: 1.0\n",
            "Epoch 1300/2000, Loss: 0.05559863895177841\n",
            "Epoch 1300/2000, Accuracy: 1.0\n",
            "Epoch 1400/2000, Loss: 0.09734416753053665\n",
            "Epoch 1400/2000, Accuracy: 0.96875\n",
            "Epoch 1500/2000, Loss: 0.07514601200819016\n",
            "Epoch 1500/2000, Accuracy: 0.96875\n",
            "Epoch 1600/2000, Loss: 0.044932980090379715\n",
            "Epoch 1600/2000, Accuracy: 1.0\n",
            "Epoch 1700/2000, Loss: 0.08858291804790497\n",
            "Epoch 1700/2000, Accuracy: 0.984375\n",
            "Epoch 1800/2000, Loss: 0.09982083737850189\n",
            "Epoch 1800/2000, Accuracy: 0.9375\n",
            "Epoch 1900/2000, Loss: 0.07204713672399521\n",
            "Epoch 1900/2000, Accuracy: 0.96875\n",
            "Epoch 2000/2000, Loss: 0.024858299642801285\n",
            "Epoch 2000/2000, Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "probe = nn.Linear(d_model, 1)\n",
        "optimizer = torch.optim.Adam(probe.parameters(), lr=learning_rate)\n",
        "\n",
        "train_probe(model, probe, optimizer, batch_size, sequence_length, num_epochs, device, get_first_attention_output_at_positions)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dnn-hw2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
