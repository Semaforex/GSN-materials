{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1ghWaLRFSGR"
   },
   "source": [
    "# Patch pooling (10pts)\n",
    "Your task is to implement patch pooling. Patch pooling takes an input sequence $(a_0, a_1, \\ldots, a_{B-1})$ of $D$-dimensional embeddings and output an output sequence $(b_0, \\ldots, b_{k-1})$ of $D$-dimensional embeddings. The length of the output sequence is not longer than the length of the input sequence and is bounded by $P$. Each element of the input sequence is called a token. Each element of the output sequence is called a patch. Consecutive patches are constructed as a mean pooling of consecutive contiguous token spans.\n",
    "\n",
    "You are given two tensors:\n",
    "1. `batch` - a $3$-dimensional tensor, which is an input to a standard transformer model with the following dimensions:\n",
    "* B - batch size\n",
    "* S - sequence lenght\n",
    "* D - dimension of embedding of a single token\n",
    "\n",
    "`batch[x,y,:]` is the embedding of the $y+1$-th token of the $x+1$-th sequence in the `batch`.\n",
    "\n",
    "2. `patch_lengths` - $2$-dimensional integer-valued tensor with the following dimensions:\n",
    "* B - batch size\n",
    "* P - maximal number of patches\n",
    "\n",
    "`patch_lengths[x,y]` is the number of tokens forming patch number $y+1$ in the $x+1$-th sequence in the `batch`.\n",
    "\n",
    "The output should be a $3$-dimensional tensor with batch of sequences of patch embeddings.\n",
    "\n",
    "# Example\n",
    "The following snippet\n",
    "```python\n",
    "batch = torch.tensor([[[ 1.,  1.,  1.,  1.,  1.],\n",
    "         [ 1.,  1.,  1.,  1.,  1.],\n",
    "         [ 1.,  1.,  1.,  1.,  1.],\n",
    "         [ 2.,  2.,  2.,  2.,  2.],\n",
    "         [ 3.,  3.,  3.,  3.,  3.],\n",
    "         [ 3.,  3.,  3.,  3.,  3.]],\n",
    "\n",
    "        [[ 4.,  4.,  4.,  4.,  4.],\n",
    "         [ 4.,  4.,  4.,  4.,  4.],\n",
    "         [ 4.,  4.,  4.,  4.,  4.],\n",
    "         [ 4.,  4.,  4.,  4.,  4.],\n",
    "         [ 5.,  5.,  5.,  5.,  5.],\n",
    "         [-1., -1., -1., -1., -1.]],\n",
    "\n",
    "        [[ 6.,  6.,  6.,  6.,  6.],\n",
    "         [-1., -1., -1., -1., -1.],\n",
    "         [-1., -1., -1., -1., -1.],\n",
    "         [-1., -1., -1., -1., -1.],\n",
    "         [-1., -1., -1., -1., -1.],\n",
    "         [-1., -1., -1., -1., -1.]]])\n",
    "patch_lengths = torch.tensor([[3, 1, 2],\n",
    "        [4, 1, 0],\n",
    "        [1, 0, 0]])\n",
    "patch_pooling = PatchPooling()\n",
    "output = patch_pooling(batch, patch_lengths)\n",
    "output\n",
    "```\n",
    "\n",
    "should ouptut\n",
    "\n",
    "```python\n",
    "torch.tensor([[[1., 1., 1., 1., 1.],\n",
    "         [2., 2., 2., 2., 2.],\n",
    "         [3., 3., 3., 3., 3.]],\n",
    "\n",
    "        [[4., 4., 4., 4., 4.],\n",
    "         [5., 5., 5., 5., 5.],\n",
    "         [-1., -1., -1., -1., -1.]],\n",
    "\n",
    "        [[6., 6., 6., 6., 6.],\n",
    "         [-1., -1., -1., -1., -1.],\n",
    "         [-1., -1., -1., -1., -1.]]])\n",
    "```\n",
    "\n",
    "Remarks:\n",
    "\n",
    "1. In this problem you can assume that embeddings of the padding token are vectors with all coordinates equal to $-1$.\n",
    "\n",
    "2. Solutions will be graded with unit tests. You are given a single test case, which will be a part of evaluation.\n",
    "\n",
    "3. Solutions not satisfying the below requirements will be graded up to 4pts:\n",
    "* You are not allowed to call custom python functions\n",
    "* You are not allowed to use Python loops\n",
    "* Your are not allowed to use any other imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4NPA3vycFSGV",
    "outputId": "ebfcf5d7-6b4c-4100-fbb2-d9e559c2bd10"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pytest\n",
    "import torch\n",
    "\n",
    "\n",
    "class PatchPooling(torch.nn.Module):\n",
    "    def forward(self, batch: torch.Tensor, patch_lengths: torch.Tensor) -> torch.Tensor:\n",
    "        B, S, D = batch.shape\n",
    "        B_1, P = patch_lengths.shape\n",
    "\n",
    "        assert B == B_1\n",
    "\n",
    "        ### Your code goes here ###\n",
    "\n",
    "        patch_ends = torch.cumsum(input = patch_lengths, dim = 1)\n",
    "        patch_beginings = patch_ends-patch_lengths\n",
    "        indexes = torch.arange(0, S)\n",
    "\n",
    "        mask = (indexes.view((1, 1, -1)) < patch_ends.view((B, P, 1))) & \\\n",
    "            (indexes.view((1, 1, -1)) >= patch_beginings.view((B, P, 1)))\n",
    "\n",
    "        patch_sum = torch.matmul(mask, batch)\n",
    "\n",
    "        patch_lengths = torch.where(patch_lengths==0, 1, patch_lengths).unsqueeze(-1)\n",
    "        safe_lengths = patch_lengths.clamp(min=1.0)\n",
    "        result = patch_sum/safe_lengths\n",
    "        result = torch.where(patch_lengths > 0, result, -1*torch.ones_like(result))\n",
    "\n",
    "        return result\n",
    "\n",
    "        ###########################\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "class TestPatchPooling:\n",
    "    @pytest.mark.parametrize(\n",
    "        \"batch,patch_lengths,expected_output\",\n",
    "        [\n",
    "            (\n",
    "                torch.tensor(\n",
    "                    [\n",
    "                        [\n",
    "                            [1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "                            [1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "                            [1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "                            [2.0, 2.0, 2.0, 2.0, 2.0],\n",
    "                            [3.0, 3.0, 3.0, 3.0, 3.0],\n",
    "                            [3.0, 3.0, 3.0, 3.0, 3.0],\n",
    "                        ],\n",
    "                        [\n",
    "                            [4.0, 4.0, 4.0, 4.0, 4.0],\n",
    "                            [4.0, 4.0, 4.0, 4.0, 4.0],\n",
    "                            [4.0, 4.0, 4.0, 4.0, 4.0],\n",
    "                            [4.0, 4.0, 4.0, 4.0, 4.0],\n",
    "                            [5.0, 5.0, 5.0, 5.0, 5.0],\n",
    "                            [-1.0, -1.0, -1.0, -1.0, -1.0],\n",
    "                        ],\n",
    "                        [\n",
    "                            [6.0, 6.0, 6.0, 6.0, 6.0],\n",
    "                            [-1.0, -1.0, -1.0, -1.0, -1.0],\n",
    "                            [-1.0, -1.0, -1.0, -1.0, -1.0],\n",
    "                            [-1.0, -1.0, -1.0, -1.0, -1.0],\n",
    "                            [-1.0, -1.0, -1.0, -1.0, -1.0],\n",
    "                            [-1.0, -1.0, -1.0, -1.0, -1.0],\n",
    "                        ],\n",
    "                    ]\n",
    "                ),\n",
    "                torch.tensor([[3, 1, 2], [4, 1, 0], [1, 0, 0]]),\n",
    "                torch.tensor(\n",
    "                    [\n",
    "                        [\n",
    "                            [1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "                            [2.0, 2.0, 2.0, 2.0, 2.0],\n",
    "                            [3.0, 3.0, 3.0, 3.0, 3.0],\n",
    "                        ],\n",
    "                        [\n",
    "                            [4.0, 4.0, 4.0, 4.0, 4.0],\n",
    "                            [5.0, 5.0, 5.0, 5.0, 5.0],\n",
    "                            [-1.0, -1.0, -1.0, -1.0, -1.0],\n",
    "                        ],\n",
    "                        [\n",
    "                            [6.0, 6.0, 6.0, 6.0, 6.0],\n",
    "                            [-1.0, -1.0, -1.0, -1.0, -1.0],\n",
    "                            [-1.0, -1.0, -1.0, -1.0, -1.0],\n",
    "                        ],\n",
    "                    ]\n",
    "                ),\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "    def test_forward(\n",
    "        self,\n",
    "        batch: torch.Tensor,\n",
    "        patch_lengths: torch.Tensor,\n",
    "        expected_output: torch.Tensor,\n",
    "    ) -> None:\n",
    "        # given\n",
    "        patch_pooling = PatchPooling()\n",
    "\n",
    "        # when\n",
    "        output = patch_pooling(batch=batch, patch_lengths=patch_lengths)\n",
    "\n",
    "        # then\n",
    "        assert torch.all(torch.isclose(output, expected_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch.tensor([\n",
    "    [[0.0], [0.0]]  # Sequence 0: Two tokens, both are 0.0\n",
    "])\n",
    "\n",
    "# Lengths: Patch 1 has length 2.\n",
    "patch_lengths = torch.tensor([[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.]]])\n"
     ]
    }
   ],
   "source": [
    "B, S, D = batch.shape\n",
    "B_1, P = patch_lengths.shape\n",
    "\n",
    "assert B == B_1\n",
    "\n",
    "        ### Your code goes here ###\n",
    "\n",
    "patch_ends = torch.cumsum(input = patch_lengths, dim = 1)\n",
    "patch_beginings = patch_ends-patch_lengths\n",
    "indexes = torch.arange(0, S)\n",
    "\n",
    "mask = (indexes.view((1, 1, -1)) < patch_ends.view((B, P, 1))) & \\\n",
    "    (indexes.view((1, 1, -1)) >= patch_beginings.view((B, P, 1)))\n",
    "\n",
    "mask = mask.to(batch.dtype)\n",
    "\n",
    "patch_sum = torch.matmul(mask, batch)\n",
    "\n",
    "patch_lengths = torch.where(patch_lengths==0, 1, patch_lengths).unsqueeze(-1)\n",
    "safe_lengths = patch_lengths.clamp(min=1.0)\n",
    "result = patch_sum/safe_lengths\n",
    "result = torch.where(patch_lengths > 0, result, -1*torch.ones_like(result))\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tI__yScxFSGX",
    "outputId": "c64f7cf3-44c9-4f49-b395-3a1894e6ec9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.12.12, pytest-9.0.2, pluggy-1.6.0\n",
      "rootdir: /home/mparadow/Desktop/GSN_egzamin_materia≈Çy/2024-25\n",
      "plugins: ament-copyright-0.14.4, ament-xmllint-0.14.4, ament-flake8-0.14.4, launch-testing-ros-0.24.2, launch-testing-2.0.4, ament-pep257-0.14.4, ament-lint-0.14.4, anyio-4.12.0\n",
      "collected 1 item                                                               \u001b[0m\u001b[1m\n",
      "\n",
      "test_patch_pooling.py \u001b[32m.\u001b[0m\u001b[32m                                                  [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.66s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest test_patch_pooling.py"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dnn-hw2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
