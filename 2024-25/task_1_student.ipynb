{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1ghWaLRFSGR"
   },
   "source": [
    "# Patch pooling (10pts)\n",
    "Your task is to implement patch pooling. Patch pooling takes an input sequence $(a_0, a_1, \\ldots, a_{B-1})$ of $D$-dimensional embeddings and output an output sequence $(b_0, \\ldots, b_{k-1})$ of $D$-dimensional embeddings. The length of the output sequence is not longer than the length of the input sequence and is bounded by $P$. Each element of the input sequence is called a token. Each element of the output sequence is called a patch. Consecutive patches are constructed as a mean pooling of consecutive contiguous token spans.\n",
    "\n",
    "You are given two tensors:\n",
    "1. `batch` - a $3$-dimensional tensor, which is an input to a standard transformer model with the following dimensions:\n",
    "* B - batch size\n",
    "* S - sequence lenght\n",
    "* D - dimension of embedding of a single token\n",
    "\n",
    "`batch[x,y,:]` is the embedding of the $y+1$-th token of the $x+1$-th sequence in the `batch`.\n",
    "\n",
    "2. `patch_lengths` - $2$-dimensional integer-valued tensor with the following dimensions:\n",
    "* B - batch size\n",
    "* P - maximal number of patches\n",
    "\n",
    "`patch_lengths[x,y]` is the number of tokens forming patch number $y+1$ in the $x+1$-th sequence in the `batch`.\n",
    "\n",
    "The output should be a $3$-dimensional tensor with batch of sequences of patch embeddings.\n",
    "\n",
    "# Example\n",
    "The following snippet\n",
    "```python\n",
    "batch = torch.tensor([[[ 1.,  1.,  1.,  1.,  1.],\n",
    "         [ 1.,  1.,  1.,  1.,  1.],\n",
    "         [ 1.,  1.,  1.,  1.,  1.],\n",
    "         [ 2.,  2.,  2.,  2.,  2.],\n",
    "         [ 3.,  3.,  3.,  3.,  3.],\n",
    "         [ 3.,  3.,  3.,  3.,  3.]],\n",
    "\n",
    "        [[ 4.,  4.,  4.,  4.,  4.],\n",
    "         [ 4.,  4.,  4.,  4.,  4.],\n",
    "         [ 4.,  4.,  4.,  4.,  4.],\n",
    "         [ 4.,  4.,  4.,  4.,  4.],\n",
    "         [ 5.,  5.,  5.,  5.,  5.],\n",
    "         [-1., -1., -1., -1., -1.]],\n",
    "\n",
    "        [[ 6.,  6.,  6.,  6.,  6.],\n",
    "         [-1., -1., -1., -1., -1.],\n",
    "         [-1., -1., -1., -1., -1.],\n",
    "         [-1., -1., -1., -1., -1.],\n",
    "         [-1., -1., -1., -1., -1.],\n",
    "         [-1., -1., -1., -1., -1.]]])\n",
    "patch_lengths = torch.tensor([[3, 1, 2],\n",
    "        [4, 1, 0],\n",
    "        [1, 0, 0]])\n",
    "patch_pooling = PatchPooling()\n",
    "output = patch_pooling(batch, patch_lengths)\n",
    "output\n",
    "```\n",
    "\n",
    "should ouptut\n",
    "\n",
    "```python\n",
    "torch.tensor([[[1., 1., 1., 1., 1.],\n",
    "         [2., 2., 2., 2., 2.],\n",
    "         [3., 3., 3., 3., 3.]],\n",
    "\n",
    "        [[4., 4., 4., 4., 4.],\n",
    "         [5., 5., 5., 5., 5.],\n",
    "         [-1., -1., -1., -1., -1.]],\n",
    "\n",
    "        [[6., 6., 6., 6., 6.],\n",
    "         [-1., -1., -1., -1., -1.],\n",
    "         [-1., -1., -1., -1., -1.]]])\n",
    "```\n",
    "\n",
    "Remarks:\n",
    "\n",
    "1. In this problem you can assume that embeddings of the padding token are vectors with all coordinates equal to $-1$.\n",
    "\n",
    "2. Solutions will be graded with unit tests. You are given a single test case, which will be a part of evaluation.\n",
    "\n",
    "3. Solutions not satisfying the below requirements will be graded up to 4pts:\n",
    "* You are not allowed to call custom python functions\n",
    "* You are not allowed to use Python loops\n",
    "* Your are not allowed to use any other imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4NPA3vycFSGV",
    "outputId": "ebfcf5d7-6b4c-4100-fbb2-d9e559c2bd10"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pytest\n",
    "import torch\n",
    "\n",
    "\n",
    "class PatchPooling(torch.nn.Module):\n",
    "    def forward(self, batch: torch.Tensor, patch_lengths: torch.Tensor) -> torch.Tensor:\n",
    "        B, S, D = batch.shape\n",
    "        B_1, P = patch_lengths.shape\n",
    "\n",
    "        assert B == B_1\n",
    "\n",
    "        ### Your code goes here ###\n",
    "\n",
    "        patch_ends = torch.cumsum(input = patch_lengths, dim = 1)\n",
    "        patch_beginings = patch_ends-patch_lengths\n",
    "        indexes = torch.arange(0, S)\n",
    "\n",
    "        mask = (indexes.view((1, 1, -1)) < patch_ends.view((B, P, 1))) & \\\n",
    "            (indexes.view((1, 1, -1)) >= patch_beginings.view((B, P, 1)))\n",
    "\n",
    "        patch_sum = torch.matmul(mask, batch)\n",
    "\n",
    "        patch_lengths = torch.where(patch_lengths==0, 1, patch_lengths).unsqueeze(-1)\n",
    "        safe_lengths = patch_lengths.clamp(min=1.0)\n",
    "        result = patch_sum/safe_lengths\n",
    "        result = torch.where(patch_lengths > 0, result, -1*torch.ones_like(result))\n",
    "\n",
    "        return result\n",
    "\n",
    "        ###########################\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "class TestPatchPooling:\n",
    "    @pytest.mark.parametrize(\n",
    "        \"batch,patch_lengths,expected_output\",\n",
    "        [\n",
    "            (\n",
    "                torch.tensor(\n",
    "                    [\n",
    "                        [\n",
    "                            [1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "                            [1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "                            [1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "                            [2.0, 2.0, 2.0, 2.0, 2.0],\n",
    "                            [3.0, 3.0, 3.0, 3.0, 3.0],\n",
    "                            [3.0, 3.0, 3.0, 3.0, 3.0],\n",
    "                        ],\n",
    "                        [\n",
    "                            [4.0, 4.0, 4.0, 4.0, 4.0],\n",
    "                            [4.0, 4.0, 4.0, 4.0, 4.0],\n",
    "                            [4.0, 4.0, 4.0, 4.0, 4.0],\n",
    "                            [4.0, 4.0, 4.0, 4.0, 4.0],\n",
    "                            [5.0, 5.0, 5.0, 5.0, 5.0],\n",
    "                            [-1.0, -1.0, -1.0, -1.0, -1.0],\n",
    "                        ],\n",
    "                        [\n",
    "                            [6.0, 6.0, 6.0, 6.0, 6.0],\n",
    "                            [-1.0, -1.0, -1.0, -1.0, -1.0],\n",
    "                            [-1.0, -1.0, -1.0, -1.0, -1.0],\n",
    "                            [-1.0, -1.0, -1.0, -1.0, -1.0],\n",
    "                            [-1.0, -1.0, -1.0, -1.0, -1.0],\n",
    "                            [-1.0, -1.0, -1.0, -1.0, -1.0],\n",
    "                        ],\n",
    "                    ]\n",
    "                ),\n",
    "                torch.tensor([[3, 1, 2], [4, 1, 0], [1, 0, 0]]),\n",
    "                torch.tensor(\n",
    "                    [\n",
    "                        [\n",
    "                            [1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "                            [2.0, 2.0, 2.0, 2.0, 2.0],\n",
    "                            [3.0, 3.0, 3.0, 3.0, 3.0],\n",
    "                        ],\n",
    "                        [\n",
    "                            [4.0, 4.0, 4.0, 4.0, 4.0],\n",
    "                            [5.0, 5.0, 5.0, 5.0, 5.0],\n",
    "                            [-1.0, -1.0, -1.0, -1.0, -1.0],\n",
    "                        ],\n",
    "                        [\n",
    "                            [6.0, 6.0, 6.0, 6.0, 6.0],\n",
    "                            [-1.0, -1.0, -1.0, -1.0, -1.0],\n",
    "                            [-1.0, -1.0, -1.0, -1.0, -1.0],\n",
    "                        ],\n",
    "                    ]\n",
    "                ),\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "    def test_forward(\n",
    "        self,\n",
    "        batch: torch.Tensor,\n",
    "        patch_lengths: torch.Tensor,\n",
    "        expected_output: torch.Tensor,\n",
    "    ) -> None:\n",
    "        # given\n",
    "        patch_pooling = PatchPooling()\n",
    "\n",
    "        # when\n",
    "        output = patch_pooling(batch=batch, patch_lengths=patch_lengths)\n",
    "\n",
    "        # then\n",
    "        assert torch.all(torch.isclose(output, expected_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch.tensor([\n",
    "    [[0.0], [0.0]]  # Sequence 0: Two tokens, both are 0.0\n",
    "])\n",
    "\n",
    "# Lengths: Patch 1 has length 2.\n",
    "patch_lengths = torch.tensor([[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.]]])\n"
     ]
    }
   ],
   "source": [
    "B, S, D = batch.shape\n",
    "B_1, P = patch_lengths.shape\n",
    "\n",
    "assert B == B_1\n",
    "\n",
    "        ### Your code goes here ###\n",
    "\n",
    "patch_ends = torch.cumsum(input = patch_lengths, dim = 1)\n",
    "patch_beginings = patch_ends-patch_lengths\n",
    "indexes = torch.arange(0, S)\n",
    "\n",
    "mask = (indexes.view((1, 1, -1)) < patch_ends.view((B, P, 1))) & \\\n",
    "    (indexes.view((1, 1, -1)) >= patch_beginings.view((B, P, 1)))\n",
    "\n",
    "mask = mask.to(batch.dtype)\n",
    "\n",
    "patch_sum = torch.matmul(mask, batch)\n",
    "\n",
    "patch_lengths = torch.where(patch_lengths==0, 1, patch_lengths).unsqueeze(-1)\n",
    "safe_lengths = patch_lengths.clamp(min=1.0)\n",
    "result = patch_sum/safe_lengths\n",
    "result = torch.where(patch_lengths > 0, result, -1*torch.ones_like(result))\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tI__yScxFSGX",
    "outputId": "c64f7cf3-44c9-4f49-b395-3a1894e6ec9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.10.19, pytest-9.0.2, pluggy-1.5.0\n",
      "rootdir: c:\\Users\\Julka\\GSN-materials\\2024-25\n",
      "collected 1 item\n",
      "\n",
      "test_patch_pooling.py \u001b[31mF\u001b[0m\u001b[31m                                                  [100%]\u001b[0m\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m____ TestPatchPooling.test_forward[batch0-patch_lengths0-expected_output0] ____\u001b[0m\n",
      "\n",
      "self = <test_patch_pooling.TestPatchPooling object at 0x000001DF7CAD4E80>\n",
      "batch = tensor([[[ 1.,  1.,  1.,  1.,  1.],\n",
      "         [ 1.,  1.,  1.,  1.,  1.],\n",
      "         [ 1.,  1.,  1.,  1.,  1.],\n",
      "         [...1., -1.],\n",
      "         [-1., -1., -1., -1., -1.],\n",
      "         [-1., -1., -1., -1., -1.],\n",
      "         [-1., -1., -1., -1., -1.]]])\n",
      "patch_lengths = tensor([[3, 1, 2],\n",
      "        [4, 1, 0],\n",
      "        [1, 0, 0]])\n",
      "expected_output = tensor([[[ 1.,  1.,  1.,  1.,  1.],\n",
      "         [ 2.,  2.,  2.,  2.,  2.],\n",
      "         [ 3.,  3.,  3.,  3.,  3.]],\n",
      "\n",
      "        ..., -1.]],\n",
      "\n",
      "        [[ 6.,  6.,  6.,  6.,  6.],\n",
      "         [-1., -1., -1., -1., -1.],\n",
      "         [-1., -1., -1., -1., -1.]]])\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mbatch,patch_lengths,expected_output\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        [\u001b[90m\u001b[39;49;00m\n",
      "            (\u001b[90m\u001b[39;49;00m\n",
      "                torch.tensor(\u001b[90m\u001b[39;49;00m\n",
      "                    [\u001b[90m\u001b[39;49;00m\n",
      "                        [\u001b[90m\u001b[39;49;00m\n",
      "                            [\u001b[94m1.0\u001b[39;49;00m, \u001b[94m1.0\u001b[39;49;00m, \u001b[94m1.0\u001b[39;49;00m, \u001b[94m1.0\u001b[39;49;00m, \u001b[94m1.0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                            [\u001b[94m1.0\u001b[39;49;00m, \u001b[94m1.0\u001b[39;49;00m, \u001b[94m1.0\u001b[39;49;00m, \u001b[94m1.0\u001b[39;49;00m, \u001b[94m1.0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                            [\u001b[94m1.0\u001b[39;49;00m, \u001b[94m1.0\u001b[39;49;00m, \u001b[94m1.0\u001b[39;49;00m, \u001b[94m1.0\u001b[39;49;00m, \u001b[94m1.0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                            [\u001b[94m2.0\u001b[39;49;00m, \u001b[94m2.0\u001b[39;49;00m, \u001b[94m2.0\u001b[39;49;00m, \u001b[94m2.0\u001b[39;49;00m, \u001b[94m2.0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                            [\u001b[94m3.0\u001b[39;49;00m, \u001b[94m3.0\u001b[39;49;00m, \u001b[94m3.0\u001b[39;49;00m, \u001b[94m3.0\u001b[39;49;00m, \u001b[94m3.0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                            [\u001b[94m3.0\u001b[39;49;00m, \u001b[94m3.0\u001b[39;49;00m, \u001b[94m3.0\u001b[39;49;00m, \u001b[94m3.0\u001b[39;49;00m, \u001b[94m3.0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                        ],\u001b[90m\u001b[39;49;00m\n",
      "                        [\u001b[90m\u001b[39;49;00m\n",
      "                            [\u001b[94m4.0\u001b[39;49;00m, \u001b[94m4.0\u001b[39;49;00m, \u001b[94m4.0\u001b[39;49;00m, \u001b[94m4.0\u001b[39;49;00m, \u001b[94m4.0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                            [\u001b[94m4.0\u001b[39;49;00m, \u001b[94m4.0\u001b[39;49;00m, \u001b[94m4.0\u001b[39;49;00m, \u001b[94m4.0\u001b[39;49;00m, \u001b[94m4.0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                            [\u001b[94m4.0\u001b[39;49;00m, \u001b[94m4.0\u001b[39;49;00m, \u001b[94m4.0\u001b[39;49;00m, \u001b[94m4.0\u001b[39;49;00m, \u001b[94m4.0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                            [\u001b[94m4.0\u001b[39;49;00m, \u001b[94m4.0\u001b[39;49;00m, \u001b[94m4.0\u001b[39;49;00m, \u001b[94m4.0\u001b[39;49;00m, \u001b[94m4.0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                            [\u001b[94m5.0\u001b[39;49;00m, \u001b[94m5.0\u001b[39;49;00m, \u001b[94m5.0\u001b[39;49;00m, \u001b[94m5.0\u001b[39;49;00m, \u001b[94m5.0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                            [-\u001b[94m1.0\u001b[39;49;00m, -\u001b[94m1.0\u001b[39;49;00m, -\u001b[94m1.0\u001b[39;49;00m, -\u001b[94m1.0\u001b[39;49;00m, -\u001b[94m1.0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                        ],\u001b[90m\u001b[39;49;00m\n",
      "                        [\u001b[90m\u001b[39;49;00m\n",
      "                            [\u001b[94m6.0\u001b[39;49;00m, \u001b[94m6.0\u001b[39;49;00m, \u001b[94m6.0\u001b[39;49;00m, \u001b[94m6.0\u001b[39;49;00m, \u001b[94m6.0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                            [-\u001b[94m1.0\u001b[39;49;00m, -\u001b[94m1.0\u001b[39;49;00m, -\u001b[94m1.0\u001b[39;49;00m, -\u001b[94m1.0\u001b[39;49;00m, -\u001b[94m1.0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                            [-\u001b[94m1.0\u001b[39;49;00m, -\u001b[94m1.0\u001b[39;49;00m, -\u001b[94m1.0\u001b[39;49;00m, -\u001b[94m1.0\u001b[39;49;00m, -\u001b[94m1.0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                            [-\u001b[94m1.0\u001b[39;49;00m, -\u001b[94m1.0\u001b[39;49;00m, -\u001b[94m1.0\u001b[39;49;00m, -\u001b[94m1.0\u001b[39;49;00m, -\u001b[94m1.0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                            [-\u001b[94m1.0\u001b[39;49;00m, -\u001b[94m1.0\u001b[39;49;00m, -\u001b[94m1.0\u001b[39;49;00m, -\u001b[94m1.0\u001b[39;49;00m, -\u001b[94m1.0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                            [-\u001b[94m1.0\u001b[39;49;00m, -\u001b[94m1.0\u001b[39;49;00m, -\u001b[94m1.0\u001b[39;49;00m, -\u001b[94m1.0\u001b[39;49;00m, -\u001b[94m1.0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                        ],\u001b[90m\u001b[39;49;00m\n",
      "                    ]\u001b[90m\u001b[39;49;00m\n",
      "                ),\u001b[90m\u001b[39;49;00m\n",
      "                torch.tensor([[\u001b[94m3\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m], [\u001b[94m4\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m], [\u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m]]),\u001b[90m\u001b[39;49;00m\n",
      "                torch.tensor(\u001b[90m\u001b[39;49;00m\n",
      "                    [\u001b[90m\u001b[39;49;00m\n",
      "                        [\u001b[90m\u001b[39;49;00m\n",
      "                            [\u001b[94m1.0\u001b[39;49;00m, \u001b[94m1.0\u001b[39;49;00m, \u001b[94m1.0\u001b[39;49;00m, \u001b[94m1.0\u001b[39;49;00m, \u001b[94m1.0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                            [\u001b[94m2.0\u001b[39;49;00m, \u001b[94m2.0\u001b[39;49;00m, \u001b[94m2.0\u001b[39;49;00m, \u001b[94m2.0\u001b[39;49;00m, \u001b[94m2.0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                            [\u001b[94m3.0\u001b[39;49;00m, \u001b[94m3.0\u001b[39;49;00m, \u001b[94m3.0\u001b[39;49;00m, \u001b[94m3.0\u001b[39;49;00m, \u001b[94m3.0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                        ],\u001b[90m\u001b[39;49;00m\n",
      "                        [\u001b[90m\u001b[39;49;00m\n",
      "                            [\u001b[94m4.0\u001b[39;49;00m, \u001b[94m4.0\u001b[39;49;00m, \u001b[94m4.0\u001b[39;49;00m, \u001b[94m4.0\u001b[39;49;00m, \u001b[94m4.0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                            [\u001b[94m5.0\u001b[39;49;00m, \u001b[94m5.0\u001b[39;49;00m, \u001b[94m5.0\u001b[39;49;00m, \u001b[94m5.0\u001b[39;49;00m, \u001b[94m5.0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                            [-\u001b[94m1.0\u001b[39;49;00m, -\u001b[94m1.0\u001b[39;49;00m, -\u001b[94m1.0\u001b[39;49;00m, -\u001b[94m1.0\u001b[39;49;00m, -\u001b[94m1.0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                        ],\u001b[90m\u001b[39;49;00m\n",
      "                        [\u001b[90m\u001b[39;49;00m\n",
      "                            [\u001b[94m6.0\u001b[39;49;00m, \u001b[94m6.0\u001b[39;49;00m, \u001b[94m6.0\u001b[39;49;00m, \u001b[94m6.0\u001b[39;49;00m, \u001b[94m6.0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                            [-\u001b[94m1.0\u001b[39;49;00m, -\u001b[94m1.0\u001b[39;49;00m, -\u001b[94m1.0\u001b[39;49;00m, -\u001b[94m1.0\u001b[39;49;00m, -\u001b[94m1.0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                            [-\u001b[94m1.0\u001b[39;49;00m, -\u001b[94m1.0\u001b[39;49;00m, -\u001b[94m1.0\u001b[39;49;00m, -\u001b[94m1.0\u001b[39;49;00m, -\u001b[94m1.0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                        ],\u001b[90m\u001b[39;49;00m\n",
      "                    ]\u001b[90m\u001b[39;49;00m\n",
      "                ),\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "        ],\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_forward\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        batch: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
      "        patch_lengths: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
      "        expected_output: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
      "    ) -> \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# given\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        patch_pooling = PatchPooling()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# when\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       output = patch_pooling(batch=batch, patch_lengths=patch_lengths)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_patch_pooling.py\u001b[0m:102: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "\u001b[1m\u001b[31m..\\..\\miniconda3\\envs\\dnn_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m:1736: in _wrapped_call_impl\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._call_impl(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m..\\..\\miniconda3\\envs\\dnn_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m:1747: in _call_impl\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m forward_call(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "\n",
      "self = PatchPooling()\n",
      "batch = tensor([[[ 1.,  1.,  1.,  1.,  1.],\n",
      "         [ 1.,  1.,  1.,  1.,  1.],\n",
      "         [ 1.,  1.,  1.,  1.,  1.],\n",
      "         [...1., -1.],\n",
      "         [-1., -1., -1., -1., -1.],\n",
      "         [-1., -1., -1., -1., -1.],\n",
      "         [-1., -1., -1., -1., -1.]]])\n",
      "patch_lengths = tensor([[3, 1, 2],\n",
      "        [4, 1, 0],\n",
      "        [1, 0, 0]])\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mforward\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, batch: torch.Tensor, patch_lengths: torch.Tensor) -> torch.Tensor:\u001b[90m\u001b[39;49;00m\n",
      "        B, S, D = batch.shape\u001b[90m\u001b[39;49;00m\n",
      "        B_1, P = patch_lengths.shape\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m B == B_1\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### Your code goes here ###\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        patch_ends = torch.cumsum(\u001b[96minput\u001b[39;49;00m = patch_lengths, dim = \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        patch_beginings = patch_ends-patch_lengths\u001b[90m\u001b[39;49;00m\n",
      "        indexes = torch.arange(\u001b[94m0\u001b[39;49;00m, S)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        mask = (indexes.view((\u001b[94m1\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m)) < patch_ends.view((B, P, \u001b[94m1\u001b[39;49;00m))) & \\\n",
      "            (indexes.view((\u001b[94m1\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m)) >= patch_beginings.view((B, P, \u001b[94m1\u001b[39;49;00m)))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      ">       patch_sum = torch.matmul(mask, batch)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       RuntimeError: \"bmm\" not implemented for 'Bool'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_patch_pooling.py\u001b[0m:22: RuntimeError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ===========================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m test_patch_pooling.py::\u001b[1mTestPatchPooling::test_forward[batch0-patch_lengths0-expected_output0]\u001b[0m - RuntimeError: \"bmm\" not implemented for 'Bool'\n",
      "\u001b[31m============================== \u001b[31m\u001b[1m1 failed\u001b[0m\u001b[31m in 3.03s\u001b[0m\u001b[31m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest test_patch_pooling.py"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dnn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
