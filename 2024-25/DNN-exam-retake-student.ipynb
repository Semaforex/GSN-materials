{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yt6ZsxXC-Enh",
    "jukit_cell_id": "3bbXhEGl0p"
   },
   "source": [
    "<center><img src='https://drive.google.com/uc?id=1_utx_ZGclmCwNttSe40kYA6VHzNocdET' height=\"60\"></center>\n",
    "\n",
    "AI TECH - Akademia Innowacyjnych Zastosowań Technologii Cyfrowych. Program Operacyjny Polska Cyfrowa na lata 2014-2020\n",
    "<hr>\n",
    "\n",
    "<center><img src='https://drive.google.com/uc?id=1BXZ0u3562N_MqCLcekI-Ens77Kk4LpPm'></center>\n",
    "\n",
    "<center>\n",
    "Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego\n",
    "Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n",
    "Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\"\n",
    "Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ykx6csq-Enk",
    "jukit_cell_id": "MjFPoyxoBk"
   },
   "source": [
    "# Latent Space Classifier\n",
    "\n",
    "\n",
    "In this task, you will:\n",
    "* train a Variational AutoEncoder on MNIST (the code is already prepared and ready)\n",
    "* train a digit classifier on the latent space of the Variational AutoEncoder\n",
    "\n",
    "## Variational AutoEncoder - review\n",
    "Below is a quick reminder on the Variational AutoEncoder:\n",
    "\n",
    "* Let $P^*$ be the true data distribution. We have some samples from this.\n",
    "* Let $p(z)$ be a *prior* distribution over the latent space. In our model, it is a multivariate Gaussian distribution $N(0,\\mathbb{I})$.\n",
    "* Let $E(x)$ be the encoder that accepts data points as input and outputs distributions over the latent space $Z$. The produced distribution is denoted $q_\\phi(z|x)$ and is the (approximate) *posterior* distribution. In our model, this is multivariate Gaussian distribution $q_\\phi(z|x) \\sim N(\\mu, diag(\\sigma^2))$, where:\n",
    "    1. $\\phi$ are weights of the encoder network.\n",
    "    2. The Encoder network accepts data points as input and outputs $\\mu$ and $\\sigma$, which are vectors of the same length as latent space. They are used to construct the approximate posterior distribution $q_\\phi(z|x)$.\n",
    "* Let $D(z)$ be the decoder that accepts samples from the latent distribution and output parameters of the likelihood distribution $p_\\theta(x|z)$. In our model, this is the Bernoulli trial per each pixel $p_\\theta(x|z_0) \\sim Bern(p)$, where:\n",
    "    1. $\\theta$ are weights of the decoder network.\n",
    "    2. The decoder network accepts a sample from the posterior distribution $q_\\phi(z|x)$ and outputs p, which is a matrix of the shape of the input image. Each value of the matrix is the parameter $\\pi$ of the Bernoulli trial $Bern(\\pi)$ for the corresponding pixel.\n",
    "    3. Data points are clipped to only contain values 0 and 1 so that the model can be trained in the given setup.\n",
    "\n",
    "The Variational AutoEncoder works by maximizing the Evidence Lower Bound (ELBO):\n",
    "\n",
    "$$ELBO = \\mathbb{E}_{z \\sim q(z|x)} \\big[\\log p_\\theta(x|z)\\big] - \\mathbb{KL}\\big(q_\\phi(z | x) || p(z)\\big).$$\n",
    "\n",
    "Where the first term of the loss is trained via stochastic gradient descent. Whereas, the second term can be calculated analytically in our setup and is equal to the following:\n",
    "\n",
    "$$ \\mathbb{KL}\\big( \\mathcal{N}(\\mu, \\sigma^2) || \\mathcal{N}(0, 1) \\big) = \\frac12 \\big(\\sigma^2 - \\log(\\sigma^2) + \\mu^2 - 1 \\big).$$\n",
    "\n",
    "You do not need to use the formulas above, as the Variational AutoEncoder is already implemented below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational AutoEncoder - code\n",
    "The code for VAE is already completed and attached below. Run the code to train the VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5oaegYX3-Enm",
    "jukit_cell_id": "mIRhZXmXjZ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms # type: ignore\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CG4DPj87-Enn",
    "jukit_cell_id": "o6ghXwN1P4"
   },
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "test_batch_size = 1000\n",
    "epochs = 5\n",
    "lr = 5e-3\n",
    "seed = 1\n",
    "log_interval = 5\n",
    "latent_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6_fkxSGM-Enn",
    "jukit_cell_id": "1fDwZIPWsn",
    "outputId": "2e99c667-72ee-4bd5-f1a6-762e45067c78"
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "train_kwargs = {'batch_size': batch_size}\n",
    "test_kwargs = {'batch_size': test_batch_size}\n",
    "if use_cuda:\n",
    "    cuda_kwargs = {'num_workers': 1,\n",
    "                    'pin_memory': True,\n",
    "                    'shuffle': True}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "J97S4jqt-Enp",
    "jukit_cell_id": "LHCziTTTb3"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 1620) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Julka\\miniconda3\\envs\\dnn_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1243\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mc:\\Users\\Julka\\miniconda3\\envs\\dnn_env\\lib\\queue.py:179\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_empty\u001b[38;5;241m.\u001b[39mwait(remaining)\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 166\u001b[0m\n\u001b[0;32m    163\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(vae\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 166\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m     test(vae, device, test_loader)\n",
      "Cell \u001b[1;32mIn[5], line 125\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, device, train_loader, optimizer, epoch, log_interval)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain\u001b[39m(\n\u001b[0;32m    117\u001b[0m     model: nn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[0;32m    118\u001b[0m     device: torch\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    122\u001b[0m     log_interval: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m    123\u001b[0m ):\n\u001b[0;32m    124\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, _) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m    126\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    127\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\Julka\\miniconda3\\envs\\dnn_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Julka\\miniconda3\\envs\\dnn_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1451\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Julka\\miniconda3\\envs\\dnn_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1402\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1400\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m   1401\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[1;32m-> 1402\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1403\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1404\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\Julka\\miniconda3\\envs\\dnn_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1256\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1255\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1257\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1258\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 1620) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "class Binarize:\n",
    "    def __call__(self, sample: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.bernoulli(sample)\n",
    "\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), Binarize()])\n",
    "dataset1 = datasets.MNIST(\"../data\", train=True, download=True, transform=transform)\n",
    "dataset2 = datasets.MNIST(\"../data\", train=False, transform=transform)\n",
    "train_loader = DataLoader(dataset1, **train_kwargs)\n",
    "test_loader = DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "EncoderOutput = namedtuple(\"EncoderOutput\", [\"mu\", \"sigma\"])\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, linear_sizes: list[int], latent_size: int):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for in_layer_size, out_layer_size in zip(linear_sizes, linear_sizes[1:]):\n",
    "            self.layers.append(nn.Linear(in_layer_size, out_layer_size))\n",
    "            self.layers.append(nn.BatchNorm1d(out_layer_size))\n",
    "            self.layers.append(nn.ReLU())\n",
    "\n",
    "        self.last_layer_mu = nn.Linear(linear_sizes[-1], latent_size)\n",
    "        self.last_layer_sigma = nn.Linear(linear_sizes[-1], latent_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = nn.Flatten()(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        mu = self.last_layer_mu(x)\n",
    "        logsigma = self.last_layer_sigma(x)\n",
    "        return EncoderOutput(mu, torch.log(1 + torch.exp(logsigma)))\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, linear_sizes: list[int], output_size: tuple[int]):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for in_layer_size, out_layer_size in zip(linear_sizes, linear_sizes[1:]):\n",
    "            self.layers.append(nn.Linear(in_layer_size, out_layer_size))\n",
    "            self.layers.append(nn.BatchNorm1d(out_layer_size))\n",
    "            self.layers.append(nn.ReLU())\n",
    "\n",
    "        self.last_layer = nn.Sequential(\n",
    "            nn.Linear(linear_sizes[-1], output_size[0] * output_size[1]), nn.Sigmoid()\n",
    "        )\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        for layer in self.layers:\n",
    "            z = layer(z)\n",
    "\n",
    "        x = self.last_layer(z)\n",
    "\n",
    "        x = x.view(-1, 1, *self.output_size)\n",
    "        return x\n",
    "\n",
    "\n",
    "VariationalAutoEncoderOutput = namedtuple(\n",
    "    \"VariationalAutoEncoderOutput\", [\"mu\", \"sigma\", \"p\"]\n",
    ")\n",
    "\n",
    "\n",
    "class VariationalAutoEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_linear_sizes: list[int],\n",
    "        latent_size: int,\n",
    "        decoder_linear_sizes: list[int],\n",
    "        output_size: tuple[int],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(encoder_linear_sizes, latent_size)\n",
    "        self.decoder = Decoder(decoder_linear_sizes, output_size)\n",
    "        self.latent_size = latent_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        encoded = self.encoder(x)\n",
    "\n",
    "        z = torch.normal(0.0, 1.0, size=list(encoded.mu.size())).to(device)\n",
    "        z = (z * encoded.sigma) + encoded.mu\n",
    "\n",
    "        decoded = self.decoder(z)\n",
    "        return VariationalAutoEncoderOutput(encoded.mu, encoded.sigma, decoded)\n",
    "\n",
    "    def sample_latent(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        encoded = self.encoder(x)\n",
    "        z = torch.normal(0.0, 1.0, size=list(encoded.mu.size())).to(device)\n",
    "        z = (z * encoded.sigma) + encoded.mu\n",
    "\n",
    "        return z\n",
    "\n",
    "    def sample(self, sample_size: int, samples=None) -> torch.Tensor:\n",
    "        if samples is None:\n",
    "            samples = torch.normal(0.0, 1.0, size=(sample_size, self.latent_size)).to(\n",
    "                device\n",
    "            )\n",
    "\n",
    "        decoded = self.decoder(samples)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "def KL_gaussian_loss(mu, sigma):\n",
    "    return torch.sum(((sigma * sigma) - (2 * torch.log(sigma)) + (mu * mu) - 1) / 2)\n",
    "\n",
    "\n",
    "def ELBO(x, p, mu, sigma):\n",
    "    BCE = F.binary_cross_entropy(p, x, reduction=\"sum\")\n",
    "    KL = KL_gaussian_loss(mu, sigma)\n",
    "    return BCE + KL\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    device: torch.device,\n",
    "    train_loader: DataLoader,\n",
    "    optimizer: optim.Optimizer,\n",
    "    epoch: int,\n",
    "    log_interval: int,\n",
    "):\n",
    "    model.train()\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = ELBO(data, output.p, output.mu, output.sigma)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_loader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "def test(model: nn.Module, device: torch.device, test_loader: DataLoader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            loss = ELBO(data, output.p, output.mu, output.sigma)\n",
    "            test_loss = test_loss + (loss * data.size(0))\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(\"\\nTest set: Average loss: {:.4f}\\n\".format(test_loss))\n",
    "\n",
    "\n",
    "vae = VariationalAutoEncoder(\n",
    "    [28 * 28, 500, 350], latent_size, [latent_size, 350, 500], (28, 28)\n",
    ")\n",
    "vae.to(device)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(vae, device, train_loader, optimizer, epoch, log_interval)\n",
    "    test(vae, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Latent Classifier - subtasks:\n",
    "Below are all graded subtasks associated with this exam task:\n",
    "\n",
    "1. Complete the implementation of `ClassificationHead`, which, given a latent vector generated by an `Encoder` for an image $i$ (just the $\\mu$ part), predicts to which class (digit) the image $i$ belongs.\n",
    "2. Complete the implementation of `Classifier`, which, given an input image, first encodes it with a frozen pre-trained `Encoder` and then passes it through the `ClassificationHead` to generate logits for classification.\n",
    "3. Complete the implementation of `train_classifier`, which trains the `Classifier` module. To be more precise, it trains only the `ClassificationHead` of the `Classifier`. So, for an image $i$, given the output of the pre-trained `Encoder` on the image $i$ (just the $\\mu$ part), `ClassificationHead` predicts the class to which the image belongs (which digit is present on the image).\n",
    "\n",
    "Remarks:\n",
    "* To earn all points, your model should achieve an accuracy greater than 90% (see test at the end).\n",
    "* Note that not all variables should be trained, and in particular, no gradients should be propagated throughout the `Encoder`.\n",
    "* Use a proper loss function for training the `Classifier` and select appropriate training parameters to ensure the final accuracy is above 90%.\n",
    "* Do not change the code outside the following blocks \n",
    "```python3\n",
    "#### TODO ####\n",
    "\n",
    "##############\n",
    "````\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.337918\n",
      "Train Epoch: 1 [5120/60000 (8%)]\tLoss: 2.080865\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.815507\n",
      "Train Epoch: 1 [15360/60000 (25%)]\tLoss: 1.583609\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 1.350826\n",
      "Train Epoch: 1 [25600/60000 (42%)]\tLoss: 1.158430\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.983767\n",
      "Train Epoch: 1 [35840/60000 (59%)]\tLoss: 0.843813\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.722939\n",
      "Train Epoch: 1 [46080/60000 (76%)]\tLoss: 0.632330\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.580865\n",
      "Train Epoch: 1 [56320/60000 (93%)]\tLoss: 0.546133\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.498039\n",
      "Train Epoch: 2 [5120/60000 (8%)]\tLoss: 0.473734\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.454741\n",
      "Train Epoch: 2 [15360/60000 (25%)]\tLoss: 0.410986\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.437411\n",
      "Train Epoch: 2 [25600/60000 (42%)]\tLoss: 0.403063\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.372678\n",
      "Train Epoch: 2 [35840/60000 (59%)]\tLoss: 0.361283\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.392574\n",
      "Train Epoch: 2 [46080/60000 (76%)]\tLoss: 0.388354\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.348554\n",
      "Train Epoch: 2 [56320/60000 (93%)]\tLoss: 0.354530\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.325336\n",
      "Train Epoch: 3 [5120/60000 (8%)]\tLoss: 0.279841\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.320233\n",
      "Train Epoch: 3 [15360/60000 (25%)]\tLoss: 0.298943\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.313336\n",
      "Train Epoch: 3 [25600/60000 (42%)]\tLoss: 0.314027\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.326925\n",
      "Train Epoch: 3 [35840/60000 (59%)]\tLoss: 0.276638\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.304649\n",
      "Train Epoch: 3 [46080/60000 (76%)]\tLoss: 0.264843\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.253319\n",
      "Train Epoch: 3 [56320/60000 (93%)]\tLoss: 0.285504\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.267216\n",
      "Train Epoch: 4 [5120/60000 (8%)]\tLoss: 0.280305\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.245972\n",
      "Train Epoch: 4 [15360/60000 (25%)]\tLoss: 0.285164\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.266901\n",
      "Train Epoch: 4 [25600/60000 (42%)]\tLoss: 0.233203\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.237882\n",
      "Train Epoch: 4 [35840/60000 (59%)]\tLoss: 0.263325\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.270083\n",
      "Train Epoch: 4 [46080/60000 (76%)]\tLoss: 0.256556\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.265566\n",
      "Train Epoch: 4 [56320/60000 (93%)]\tLoss: 0.192252\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.222446\n",
      "Train Epoch: 5 [5120/60000 (8%)]\tLoss: 0.227089\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.225179\n",
      "Train Epoch: 5 [15360/60000 (25%)]\tLoss: 0.219409\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.223192\n",
      "Train Epoch: 5 [25600/60000 (42%)]\tLoss: 0.252555\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.232958\n",
      "Train Epoch: 5 [35840/60000 (59%)]\tLoss: 0.200860\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.223444\n",
      "Train Epoch: 5 [46080/60000 (76%)]\tLoss: 0.227056\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.240880\n",
      "Train Epoch: 5 [56320/60000 (93%)]\tLoss: 0.227607\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.220763\n",
      "Train Epoch: 6 [5120/60000 (8%)]\tLoss: 0.201546\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.218899\n",
      "Train Epoch: 6 [15360/60000 (25%)]\tLoss: 0.222061\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.179354\n",
      "Train Epoch: 6 [25600/60000 (42%)]\tLoss: 0.202657\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.218179\n",
      "Train Epoch: 6 [35840/60000 (59%)]\tLoss: 0.206125\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.239356\n",
      "Train Epoch: 6 [46080/60000 (76%)]\tLoss: 0.234145\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.197987\n",
      "Train Epoch: 6 [56320/60000 (93%)]\tLoss: 0.181448\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.198711\n",
      "Train Epoch: 7 [5120/60000 (8%)]\tLoss: 0.195598\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.200198\n",
      "Train Epoch: 7 [15360/60000 (25%)]\tLoss: 0.192453\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.191034\n",
      "Train Epoch: 7 [25600/60000 (42%)]\tLoss: 0.217827\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.214547\n",
      "Train Epoch: 7 [35840/60000 (59%)]\tLoss: 0.173595\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.208489\n",
      "Train Epoch: 7 [46080/60000 (76%)]\tLoss: 0.198503\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.190693\n",
      "Train Epoch: 7 [56320/60000 (93%)]\tLoss: 0.189829\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.201444\n",
      "Train Epoch: 8 [5120/60000 (8%)]\tLoss: 0.200755\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.162925\n",
      "Train Epoch: 8 [15360/60000 (25%)]\tLoss: 0.197113\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.164346\n",
      "Train Epoch: 8 [25600/60000 (42%)]\tLoss: 0.184497\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.234257\n",
      "Train Epoch: 8 [35840/60000 (59%)]\tLoss: 0.172348\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.183469\n",
      "Train Epoch: 8 [46080/60000 (76%)]\tLoss: 0.191148\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.191923\n",
      "Train Epoch: 8 [56320/60000 (93%)]\tLoss: 0.136146\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.180900\n",
      "Train Epoch: 9 [5120/60000 (8%)]\tLoss: 0.176031\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.194254\n",
      "Train Epoch: 9 [15360/60000 (25%)]\tLoss: 0.155169\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.195042\n",
      "Train Epoch: 9 [25600/60000 (42%)]\tLoss: 0.183524\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.159368\n",
      "Train Epoch: 9 [35840/60000 (59%)]\tLoss: 0.214789\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.190598\n",
      "Train Epoch: 9 [46080/60000 (76%)]\tLoss: 0.194096\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.176687\n",
      "Train Epoch: 9 [56320/60000 (93%)]\tLoss: 0.159755\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.188488\n",
      "Train Epoch: 10 [5120/60000 (8%)]\tLoss: 0.147251\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.167671\n",
      "Train Epoch: 10 [15360/60000 (25%)]\tLoss: 0.161977\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.170810\n",
      "Train Epoch: 10 [25600/60000 (42%)]\tLoss: 0.184767\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.150095\n",
      "Train Epoch: 10 [35840/60000 (59%)]\tLoss: 0.168909\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.212481\n",
      "Train Epoch: 10 [46080/60000 (76%)]\tLoss: 0.157251\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.205149\n",
      "Train Epoch: 10 [56320/60000 (93%)]\tLoss: 0.160463\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.152322\n",
      "Train Epoch: 11 [5120/60000 (8%)]\tLoss: 0.173042\n",
      "Train Epoch: 11 [10240/60000 (17%)]\tLoss: 0.159588\n",
      "Train Epoch: 11 [15360/60000 (25%)]\tLoss: 0.203032\n",
      "Train Epoch: 11 [20480/60000 (34%)]\tLoss: 0.171621\n",
      "Train Epoch: 11 [25600/60000 (42%)]\tLoss: 0.199033\n",
      "Train Epoch: 11 [30720/60000 (51%)]\tLoss: 0.163836\n",
      "Train Epoch: 11 [35840/60000 (59%)]\tLoss: 0.169495\n",
      "Train Epoch: 11 [40960/60000 (68%)]\tLoss: 0.165137\n",
      "Train Epoch: 11 [46080/60000 (76%)]\tLoss: 0.208399\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.167718\n",
      "Train Epoch: 11 [56320/60000 (93%)]\tLoss: 0.192036\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.183467\n",
      "Train Epoch: 12 [5120/60000 (8%)]\tLoss: 0.158137\n",
      "Train Epoch: 12 [10240/60000 (17%)]\tLoss: 0.155671\n",
      "Train Epoch: 12 [15360/60000 (25%)]\tLoss: 0.163437\n",
      "Train Epoch: 12 [20480/60000 (34%)]\tLoss: 0.145619\n",
      "Train Epoch: 12 [25600/60000 (42%)]\tLoss: 0.193547\n",
      "Train Epoch: 12 [30720/60000 (51%)]\tLoss: 0.190502\n",
      "Train Epoch: 12 [35840/60000 (59%)]\tLoss: 0.176717\n",
      "Train Epoch: 12 [40960/60000 (68%)]\tLoss: 0.160599\n",
      "Train Epoch: 12 [46080/60000 (76%)]\tLoss: 0.168130\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.148307\n",
      "Train Epoch: 12 [56320/60000 (93%)]\tLoss: 0.170413\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.179001\n",
      "Train Epoch: 13 [5120/60000 (8%)]\tLoss: 0.163591\n",
      "Train Epoch: 13 [10240/60000 (17%)]\tLoss: 0.154500\n",
      "Train Epoch: 13 [15360/60000 (25%)]\tLoss: 0.152563\n",
      "Train Epoch: 13 [20480/60000 (34%)]\tLoss: 0.178937\n",
      "Train Epoch: 13 [25600/60000 (42%)]\tLoss: 0.181470\n",
      "Train Epoch: 13 [30720/60000 (51%)]\tLoss: 0.163398\n",
      "Train Epoch: 13 [35840/60000 (59%)]\tLoss: 0.140741\n",
      "Train Epoch: 13 [40960/60000 (68%)]\tLoss: 0.160276\n",
      "Train Epoch: 13 [46080/60000 (76%)]\tLoss: 0.192689\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.166329\n",
      "Train Epoch: 13 [56320/60000 (93%)]\tLoss: 0.170227\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.168063\n",
      "Train Epoch: 14 [5120/60000 (8%)]\tLoss: 0.179650\n",
      "Train Epoch: 14 [10240/60000 (17%)]\tLoss: 0.154230\n",
      "Train Epoch: 14 [15360/60000 (25%)]\tLoss: 0.178373\n",
      "Train Epoch: 14 [20480/60000 (34%)]\tLoss: 0.125493\n",
      "Train Epoch: 14 [25600/60000 (42%)]\tLoss: 0.189155\n",
      "Train Epoch: 14 [30720/60000 (51%)]\tLoss: 0.180251\n",
      "Train Epoch: 14 [35840/60000 (59%)]\tLoss: 0.128858\n",
      "Train Epoch: 14 [40960/60000 (68%)]\tLoss: 0.185191\n",
      "Train Epoch: 14 [46080/60000 (76%)]\tLoss: 0.170460\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.191259\n",
      "Train Epoch: 14 [56320/60000 (93%)]\tLoss: 0.179726\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.161567\n",
      "Train Epoch: 15 [5120/60000 (8%)]\tLoss: 0.207109\n",
      "Train Epoch: 15 [10240/60000 (17%)]\tLoss: 0.159642\n",
      "Train Epoch: 15 [15360/60000 (25%)]\tLoss: 0.161973\n",
      "Train Epoch: 15 [20480/60000 (34%)]\tLoss: 0.176253\n",
      "Train Epoch: 15 [25600/60000 (42%)]\tLoss: 0.174799\n",
      "Train Epoch: 15 [30720/60000 (51%)]\tLoss: 0.142547\n",
      "Train Epoch: 15 [35840/60000 (59%)]\tLoss: 0.153227\n",
      "Train Epoch: 15 [40960/60000 (68%)]\tLoss: 0.195520\n",
      "Train Epoch: 15 [46080/60000 (76%)]\tLoss: 0.181546\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.175644\n",
      "Train Epoch: 15 [56320/60000 (93%)]\tLoss: 0.171648\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.171859\n",
      "Train Epoch: 16 [5120/60000 (8%)]\tLoss: 0.162442\n",
      "Train Epoch: 16 [10240/60000 (17%)]\tLoss: 0.202260\n",
      "Train Epoch: 16 [15360/60000 (25%)]\tLoss: 0.180579\n",
      "Train Epoch: 16 [20480/60000 (34%)]\tLoss: 0.173967\n",
      "Train Epoch: 16 [25600/60000 (42%)]\tLoss: 0.163123\n",
      "Train Epoch: 16 [30720/60000 (51%)]\tLoss: 0.160611\n",
      "Train Epoch: 16 [35840/60000 (59%)]\tLoss: 0.151869\n",
      "Train Epoch: 16 [40960/60000 (68%)]\tLoss: 0.173145\n",
      "Train Epoch: 16 [46080/60000 (76%)]\tLoss: 0.147990\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.190181\n",
      "Train Epoch: 16 [56320/60000 (93%)]\tLoss: 0.155188\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.158452\n",
      "Train Epoch: 17 [5120/60000 (8%)]\tLoss: 0.168956\n",
      "Train Epoch: 17 [10240/60000 (17%)]\tLoss: 0.171056\n",
      "Train Epoch: 17 [15360/60000 (25%)]\tLoss: 0.169761\n",
      "Train Epoch: 17 [20480/60000 (34%)]\tLoss: 0.183852\n",
      "Train Epoch: 17 [25600/60000 (42%)]\tLoss: 0.165269\n",
      "Train Epoch: 17 [30720/60000 (51%)]\tLoss: 0.159704\n",
      "Train Epoch: 17 [35840/60000 (59%)]\tLoss: 0.196660\n",
      "Train Epoch: 17 [40960/60000 (68%)]\tLoss: 0.142339\n",
      "Train Epoch: 17 [46080/60000 (76%)]\tLoss: 0.165698\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.159585\n",
      "Train Epoch: 17 [56320/60000 (93%)]\tLoss: 0.181493\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.136685\n",
      "Train Epoch: 18 [5120/60000 (8%)]\tLoss: 0.144656\n",
      "Train Epoch: 18 [10240/60000 (17%)]\tLoss: 0.137727\n",
      "Train Epoch: 18 [15360/60000 (25%)]\tLoss: 0.184337\n",
      "Train Epoch: 18 [20480/60000 (34%)]\tLoss: 0.153455\n",
      "Train Epoch: 18 [25600/60000 (42%)]\tLoss: 0.149771\n",
      "Train Epoch: 18 [30720/60000 (51%)]\tLoss: 0.144700\n",
      "Train Epoch: 18 [35840/60000 (59%)]\tLoss: 0.185480\n",
      "Train Epoch: 18 [40960/60000 (68%)]\tLoss: 0.122469\n",
      "Train Epoch: 18 [46080/60000 (76%)]\tLoss: 0.151594\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.145004\n",
      "Train Epoch: 18 [56320/60000 (93%)]\tLoss: 0.136205\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.192839\n",
      "Train Epoch: 19 [5120/60000 (8%)]\tLoss: 0.153778\n",
      "Train Epoch: 19 [10240/60000 (17%)]\tLoss: 0.161146\n",
      "Train Epoch: 19 [15360/60000 (25%)]\tLoss: 0.154858\n",
      "Train Epoch: 19 [20480/60000 (34%)]\tLoss: 0.153442\n",
      "Train Epoch: 19 [25600/60000 (42%)]\tLoss: 0.139528\n",
      "Train Epoch: 19 [30720/60000 (51%)]\tLoss: 0.193933\n",
      "Train Epoch: 19 [35840/60000 (59%)]\tLoss: 0.159295\n",
      "Train Epoch: 19 [40960/60000 (68%)]\tLoss: 0.138361\n",
      "Train Epoch: 19 [46080/60000 (76%)]\tLoss: 0.124904\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.144275\n",
      "Train Epoch: 19 [56320/60000 (93%)]\tLoss: 0.158285\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.149714\n",
      "Train Epoch: 20 [5120/60000 (8%)]\tLoss: 0.137847\n",
      "Train Epoch: 20 [10240/60000 (17%)]\tLoss: 0.157624\n",
      "Train Epoch: 20 [15360/60000 (25%)]\tLoss: 0.177080\n",
      "Train Epoch: 20 [20480/60000 (34%)]\tLoss: 0.163652\n",
      "Train Epoch: 20 [25600/60000 (42%)]\tLoss: 0.182358\n",
      "Train Epoch: 20 [30720/60000 (51%)]\tLoss: 0.148076\n",
      "Train Epoch: 20 [35840/60000 (59%)]\tLoss: 0.172107\n",
      "Train Epoch: 20 [40960/60000 (68%)]\tLoss: 0.146605\n",
      "Train Epoch: 20 [46080/60000 (76%)]\tLoss: 0.152129\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.150558\n",
      "Train Epoch: 20 [56320/60000 (93%)]\tLoss: 0.164476\n"
     ]
    }
   ],
   "source": [
    "class ClassifactionHead(nn.Module):  \n",
    "    def __init__(self, latent_size, num_classes):\n",
    "        super().__init__()\n",
    "        #### TODO ####\n",
    "        self.f1 = nn.Linear(latent_size, 64)\n",
    "        self.f2 = nn.Linear(64, num_classes)\n",
    "        ##############\n",
    "\n",
    "    def forward(self, x):\n",
    "        #### TODO ####\n",
    "        x = self.f1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.f2(x)\n",
    "        return x\n",
    "        ##############\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):  # 1pt\n",
    "    def __init__(self, vae, head):\n",
    "        super().__init__()\n",
    "        #### TODO ####\n",
    "        self.vae = vae\n",
    "        self.head = head\n",
    "        ##############\n",
    "\n",
    "    def forward(self, x):\n",
    "        #### TODO ####\n",
    "        x = self.vae.encoder(x)\n",
    "        x = self.head(x.mu)\n",
    "\n",
    "        return x\n",
    "        ##############\n",
    "\n",
    "\n",
    "def train_classifier(train_loader, epochs=10, **kwargs):\n",
    "    #### TODO ####\n",
    "\n",
    "    epochs = kwargs.get(\"epochs\", epochs)\n",
    "\n",
    "    num_classes = len(train_loader.dataset.classes)\n",
    "    head = ClassifactionHead(latent_size, num_classes)\n",
    "    head.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(head.parameters(), lr=kwargs[\"lr\"])\n",
    "\n",
    "    head.train()\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                z = vae.encoder(data).mu.detach()\n",
    "            \n",
    "            output = head(z)\n",
    "\n",
    "            cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "            loss = cross_entropy_loss(output, target)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print(\n",
    "                    \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                        epoch,\n",
    "                        batch_idx * len(data),\n",
    "                        len(train_loader.dataset),\n",
    "                        100.0 * batch_idx / len(train_loader),\n",
    "                        loss.item(),\n",
    "                    )\n",
    "                )\n",
    "        \n",
    "    classifier = Classifier(vae, head)\n",
    "    classifier.to(device)\n",
    "\n",
    "    return classifier\n",
    "    ##############\n",
    "\n",
    "#### TODO ####\n",
    "# Adjust kwargs if needed\n",
    "train_function_kwargs = {\"lr\": 4e-3, \"epochs\": 20}\n",
    "##############\n",
    "\n",
    "classifier = train_classifier(train_loader, **train_function_kwargs) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.9516\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_classifier(classifier):\n",
    "    classifier.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for (data, label) in test_loader:\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "            output = classifier(data)\n",
    "            loss = torch.mean((torch.argmax(output, dim=-1) == label).to(float))\n",
    "            test_loss = test_loss + (loss * data.size(0))\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}\\n'.format(test_loss))\n",
    "    return test_loss\n",
    "\n",
    "assert test_classifier(classifier) > 0.90, 'Classifier not trained well enough'\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dnn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
