{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFiew9xpybfu"
      },
      "source": [
        "# Task 1 (6p)\n",
        "Your task is to modify the custom implementation of MultiHeadAttention. This custom implementation, currently, enables each token to attent to every other token.\n",
        "\n",
        "\n",
        "Your job is to change this behavior in a specific way.\n",
        "Let $S$ be our input sequence of length $2 \\cdot k$:\n",
        "- tokens on positions $i \\lt k$ should attend to prefix of $S$ of length $k$ ($S[:k]$) - every token up to position k\n",
        "- tokens on positions $i \\ge k$ should attend to prefix of $S$  of length $i + 1$ ($S[:i + 1]$) - every previous token and itself\n",
        "\n",
        "(Note: You can assume the sequence length is always an even number)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "ENFiK_cTeDM0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "class MultiHeadAttention(torch.nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_head):\n",
        "      super().__init__()\n",
        "      self.d_model = d_model\n",
        "      self.num_heads = num_heads\n",
        "      self.d_head = d_head\n",
        "\n",
        "      self.W_Q = torch.nn.Linear(d_model, num_heads*d_head, bias=True)\n",
        "      self.W_K = torch.nn.Linear(d_model, num_heads*d_head, bias=True)\n",
        "      self.W_V = torch.nn.Linear(d_model, num_heads*d_head, bias=True)\n",
        "      self.W_O = torch.nn.Linear(num_heads*d_head, d_model, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "      seq_len, batch_size, _ = x.shape\n",
        "\n",
        "      Q = self.W_Q(x).reshape(seq_len, batch_size, self.num_heads, self.d_head)\n",
        "      K = self.W_K(x).reshape(seq_len, batch_size, self.num_heads, self.d_head)\n",
        "      V = self.W_V(x).reshape(seq_len, batch_size, self.num_heads, self.d_head)\n",
        "\n",
        "      scaled_QK = torch.einsum(\"ibhd,jbhd->bhij\", Q, K) / math.sqrt(self.d_head)\n",
        "      # shape of scaled_QK is (batch_size, num_heads, seq_len, seq_len)\n",
        "      #TODO\n",
        "\n",
        "      k = seq_len//2\n",
        "      under_k_indices = torch.ones_like(scaled_QK)\n",
        "      under_k_indices[:, :, :, k:] = 0\n",
        "      over_k_indices = torch.tril(torch.ones_like(scaled_QK))\n",
        "\n",
        "      mask = (under_k_indices+over_k_indices)#.reshape(1, 1, seq_len, seq_len)\n",
        "\n",
        "\n",
        "      scaled_QK = torch.where(mask>0, scaled_QK, float(\"-inf\"))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      #ENDTODO\n",
        "      weights = F.softmax(scaled_QK, -1)\n",
        "      attention = torch.einsum(\"bhij,jbhd->ibhd\", weights, V)\n",
        "\n",
        "      result = self.W_O(attention.reshape(seq_len, batch_size,self.num_heads * self.d_head))\n",
        "\n",
        "      return result, weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "GDQ0a57NeB-z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result: tensor([[[ 0.0372,  0.7503],\n",
            "         [ 0.2375,  0.7533]],\n",
            "\n",
            "        [[ 0.0163,  0.7479],\n",
            "         [ 0.2390,  0.7526]],\n",
            "\n",
            "        [[ 0.0351,  0.7500],\n",
            "         [ 0.2403,  0.7547]],\n",
            "\n",
            "        [[ 0.1086,  0.7195],\n",
            "         [ 0.4531,  0.8649]],\n",
            "\n",
            "        [[ 0.1411,  0.8073],\n",
            "         [ 0.1710,  0.7565]],\n",
            "\n",
            "        [[ 0.1228,  0.6960],\n",
            "         [-0.1254,  0.5422]]])\n",
            "Weights: tensor([[[[0.2962, 0.3586, 0.3451, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2755, 0.4078, 0.3167, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2939, 0.3625, 0.3437, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2130, 0.2352, 0.2527, 0.2992, 0.0000, 0.0000],\n",
            "          [0.1813, 0.2649, 0.2027, 0.1828, 0.1683, 0.0000],\n",
            "          [0.1318, 0.1333, 0.1593, 0.2053, 0.0462, 0.3242]],\n",
            "\n",
            "         [[0.3326, 0.3210, 0.3464, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3081, 0.3626, 0.3293, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3307, 0.3259, 0.3433, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2483, 0.2201, 0.2521, 0.2795, 0.0000, 0.0000],\n",
            "          [0.1960, 0.2245, 0.2158, 0.2249, 0.1388, 0.0000],\n",
            "          [0.1676, 0.1370, 0.1660, 0.1877, 0.1320, 0.2096]]],\n",
            "\n",
            "\n",
            "        [[[0.4130, 0.3034, 0.2836, 0.0000, 0.0000, 0.0000],\n",
            "          [0.4113, 0.3063, 0.2824, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3915, 0.3003, 0.3082, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3499, 0.2597, 0.2106, 0.1798, 0.0000, 0.0000],\n",
            "          [0.2285, 0.1725, 0.1868, 0.0933, 0.3189, 0.0000],\n",
            "          [0.1146, 0.0766, 0.0611, 0.0441, 0.1197, 0.5839]],\n",
            "\n",
            "         [[0.3513, 0.3365, 0.3122, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3579, 0.3381, 0.3039, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3594, 0.3268, 0.3138, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2630, 0.2510, 0.1994, 0.2866, 0.0000, 0.0000],\n",
            "          [0.2101, 0.1938, 0.2008, 0.1606, 0.2348, 0.0000],\n",
            "          [0.1614, 0.1728, 0.1562, 0.2170, 0.1341, 0.1585]]]])\n"
          ]
        }
      ],
      "source": [
        "# Test your solution\n",
        "d_model = 2\n",
        "num_heads= 2\n",
        "d_head = 4\n",
        "k = 3\n",
        "batch_size = 2\n",
        "\n",
        "mha = MultiHeadAttention(d_model, num_heads, d_head)\n",
        "batched_x= torch.randn((2*k, batch_size, d_model))\n",
        "with torch.no_grad():\n",
        "  result, weights = mha(batched_x)\n",
        "print(\"Result:\", result)\n",
        "print(\"Weights:\", weights)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dnn-hw2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
